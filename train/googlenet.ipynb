{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "In this kernel, I show how I changed the backbone of the Faster-R-CNN model from ResNet50 to ResNet152. To achieve that, I used some of the source code of the torchvision and changed it manually."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "The following points are covered:\n",
    "* Create dataset\n",
    "* Create dataloader\n",
    "* Prepare the model\n",
    "* Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN\n",
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "from torchvision.models import (inception_v3,resnext50_32x4d,squeezenet1_0, googlenet)\n",
    "\n",
    "from albumentations import *\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Prepare Input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Device(Enum):\n",
    "    GPU = \"GPU\"\n",
    "    TPU = \"TPU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_data_dir = Path(\"/kaggle/input/global-wheat-detection/\")\n",
    "test_train_ratio = 0.1\n",
    "batch_size=32\n",
    "seed = 0\n",
    "train_device = Device.GPU\n",
    "number_of_epochs = 1000\n",
    "learning_rate = 0.0001\n",
    "weight_decay = 1e-5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DatasetArguments:\n",
    "    data_dir: Path\n",
    "    images_lists_dict: dict\n",
    "    labels_csv_file_name: str\n",
    "\n",
    "@dataclass\n",
    "class DataLoaderArguments:\n",
    "    batch_size: int\n",
    "    num_workers: int\n",
    "    dataset_arguments: DatasetArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Split Data to train and val datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_images_file_names_from_csv(directory):\n",
    "    dataframe = pd.read_csv(os.path.join(directory, \"train.csv\"))\n",
    "    files = dataframe[\"image_id\"].unique().tolist()\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _choose_train_valid_file_names(file_names, valid_numbers, seed):\n",
    "    np.random.seed(seed)\n",
    "    valid_file_names = np.random.choice(file_names, valid_numbers, replace=False).tolist()\n",
    "    train_file_names = [file_name_i for file_name_i in file_names if file_name_i not in valid_file_names]\n",
    "    return train_file_names, valid_file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data\n",
    "file_names = _get_images_file_names_from_csv(root_data_dir)\n",
    "valid_numbers = round(len(file_names) * test_train_ratio)\n",
    "train_file_names, valid_file_names = _choose_train_valid_file_names(file_names, valid_numbers, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_lists_dict = {\n",
    "    \"train\": train_file_names,\n",
    "    \"val\": valid_file_names,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_arguments = DatasetArguments(\n",
    "    data_dir=root_data_dir,\n",
    "    images_lists_dict=images_lists_dict,\n",
    "    labels_csv_file_name=\"train.csv\",\n",
    ")\n",
    "\n",
    "dataloaders_arguments = DataLoaderArguments(\n",
    "    batch_size=batch_size,\n",
    "    num_workers=1,\n",
    "    dataset_arguments=dataset_arguments\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Prepare the transforms:\n",
    "I chose some of the transforms from this [notebook](https://www.kaggle.com/shonenkov/training-efficientdet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_set():\n",
    "    transforms_dict = {\n",
    "        'train': get_train_transforms(),\n",
    "        'val': get_valid_transforms()\n",
    "    }\n",
    "    return transforms_dict\n",
    "\n",
    "\n",
    "def get_train_transforms():\n",
    "    return Compose(\n",
    "        [OneOf([HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2,\n",
    "                                   val_shift_limit=0.2, p=0.9),\n",
    "                RandomBrightnessContrast(brightness_limit=0.2,\n",
    "                                         contrast_limit=0.2, p=0.9)],\n",
    "               p=0.9),\n",
    "            ToGray(p=0.01),\n",
    "            HorizontalFlip(p=0.5),\n",
    "            VerticalFlip(p=0.5),\n",
    "            Cutout(num_holes=8, max_h_size=64, max_w_size=64, fill_value=0, p=0.5),\n",
    "            ToTensorV2(p=1.0)],\n",
    "        p=1.0,\n",
    "        bbox_params=BboxParams(\n",
    "            format='pascal_voc',\n",
    "            min_area=0,\n",
    "            min_visibility=0,\n",
    "            label_fields=['labels']\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def get_valid_transforms():\n",
    "    return Compose(\n",
    "        [\n",
    "            ToTensorV2(p=1.0),\n",
    "        ],\n",
    "        p=1.0,\n",
    "        bbox_params=BboxParams(\n",
    "            format='pascal_voc',\n",
    "            min_area=0,\n",
    "            min_visibility=0,\n",
    "            label_fields=['labels']\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Create the pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _adjust_boxes_format(boxes):\n",
    "    # original format [xmin, ymin, width, height]\n",
    "    # new format [xmin, ymin, xmax, ymax]\n",
    "    adjusted_boxes = []\n",
    "    for box_i in boxes:\n",
    "        adjusted_box_i = [0, 0, 0, 0]\n",
    "        adjusted_box_i[0] = box_i[0]\n",
    "        adjusted_box_i[1] = box_i[1]\n",
    "        adjusted_box_i[2] = box_i[0] + box_i[2]\n",
    "        adjusted_box_i[3] = box_i[1] + box_i[3]\n",
    "        adjusted_boxes.append(adjusted_box_i)\n",
    "    return adjusted_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _areas(boxes):\n",
    "    # original format [xmin, ymin, width, height]\n",
    "    areas = []\n",
    "    for box_i in boxes:\n",
    "        areas.append(box_i[2] * box_i[3])\n",
    "    return areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Didn't understand\n",
    "\n",
    "\"\"\"\n",
    "# dataset\n",
    "class ObjectDetectionDataset(Dataset):\n",
    "    def __init__(self, images_root_directory,\n",
    "                 images_list,\n",
    "                 labels_csv_file_name,\n",
    "                 phase,\n",
    "                 transforms):\n",
    "        super(ObjectDetectionDataset).__init__()\n",
    "        self.images_root_directory = images_root_directory\n",
    "        self.phase = phase\n",
    "        self.transforms = transforms\n",
    "        self.images_list = images_list\n",
    "        if self.phase in [\"train\", \"val\"]:\n",
    "            self.labels_dataframe = pd.read_csv(os.path.join(images_root_directory, labels_csv_file_name))\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        sample = {\n",
    "            \"local_image_id\": None,\n",
    "            \"image_id\": None,\n",
    "            \"labels\": None,\n",
    "            \"boxes\": None,\n",
    "            \"area\": None,\n",
    "            \"iscrowd\": None\n",
    "        }\n",
    "\n",
    "        image_id = self.images_list[item]\n",
    "        image_path = os.path.join(self.images_root_directory,\n",
    "                                  \"train\" if self.phase in [\"train\", \"val\"] else \"test\",\n",
    "                                  image_id + \".jpg\")\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "        sample[\"local_image_id\"] = image_id\n",
    "        sample[\"image_id\"] = torch.tensor([item])\n",
    "        if self.phase in [\"train\", \"val\"]:\n",
    "            boxes = self.labels_dataframe[self.labels_dataframe.image_id == image_id].bbox.values.tolist()\n",
    "            boxes = [eval(box_i) for box_i in boxes]\n",
    "            areas = _areas(boxes)\n",
    "            boxes = _adjust_boxes_format(boxes)\n",
    "\n",
    "            sample[\"labels\"] = torch.ones((len(boxes),), dtype=torch.int64)\n",
    "            sample[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            sample[\"area\"] = torch.as_tensor(areas, dtype=torch.float32)\n",
    "            sample[\"iscrowd\"] = torch.zeros((len(boxes),), dtype=torch.int64)\n",
    "        if self.transforms is not None:\n",
    "            sample[\"image\"] = image\n",
    "            transformed_sample = self.transforms(image=sample[\"image\"],\n",
    "                                                 bboxes=sample[\"boxes\"],\n",
    "                                                 labels=sample[\"labels\"])\n",
    "            image = transformed_sample[\"image\"]\n",
    "            sample[\"boxes\"] = torch.as_tensor(transformed_sample[\"bboxes\"], dtype=torch.float32)\n",
    "            del sample[\"image\"]\n",
    "        return image, sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(arguments):\n",
    "    dataset = ObjectDetectionDataset(arguments.data_dir,\n",
    "                                     arguments.images_lists_dict[arguments.phase],\n",
    "                                     arguments.labels_csv_file_name,\n",
    "                                     arguments.phase,\n",
    "                                     arguments.transforms)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets_dictionary(arguments, input_size):\n",
    "    data_transforms = transform_set()\n",
    "    image_datasets = {\n",
    "        'train': None,\n",
    "        'val': None\n",
    "    }\n",
    "    for phase in ['train', 'val']:\n",
    "        arguments.phase = phase\n",
    "        arguments.transforms = data_transforms[phase]\n",
    "        image_datasets[phase] = create_dataset(arguments)\n",
    "    return image_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Create the pytorch dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders_dictionary(arguments, input_size):\n",
    "    batch_size = arguments.batch_size\n",
    "    num_workers = arguments.num_workers\n",
    "    image_datasets = create_datasets_dictionary(arguments.dataset_arguments, input_size)\n",
    "    dataloaders_dict = {x: DataLoader(image_datasets[x],\n",
    "                                      batch_size=batch_size,\n",
    "                                      shuffle=True,\n",
    "                                      pin_memory=True,\n",
    "                                      num_workers=num_workers,\n",
    "                                      collate_fn=collate_fn) for x in ['train', 'val']}\n",
    "    return dataloaders_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Prepare the model\n",
    "I used the code here:\n",
    "[torchvision source code](https://github.com/pytorch/vision/blob/3d65fc6723f1e0709916f24d819d6e17a925b394/torchvision/models/detection/backbone_utils.py#L44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/googlenet-1378be20.pth\" to /root/.cache/torch/checkpoints/googlenet-1378be20.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae113c736274478f8c10f63298fd4815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=52147035.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class GoogleFeatures(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GoogleFeatures, self).__init__()\n",
    "        base_model =  googlenet(pretrained=True)\n",
    "\n",
    "        self.seq1 = nn.Sequential(base_model.conv1,\n",
    "                                  base_model.maxpool1,\n",
    "                                  base_model.conv2,\n",
    "                                  base_model.conv3,\n",
    "                                  base_model.maxpool2\n",
    "                                  )\n",
    "        self.seq2 = nn.Sequential(base_model.inception3a,\n",
    "                                  base_model.inception3b,\n",
    "                                  base_model.maxpool3,\n",
    "                                  base_model.inception4a,\n",
    "                                  base_model.aux1\n",
    "                                  )\n",
    "        self.seq3 = nn.Sequential(base_model.inception4b,\n",
    "                                  base_model.inception4c,\n",
    "                                  base_model.inception4d,\n",
    "                                  base_model.aux2\n",
    "                                  )\n",
    "        self.seq4 = nn.Sequential(base_model.inception4e,\n",
    "                                  base_model.maxpool4,\n",
    "                                  base_model.inception5a,\n",
    "                                  base_model.inception5b\n",
    "                                  )\n",
    "        self.out_channels = 192\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.seq1(x)\n",
    "\n",
    "        return x\n",
    "backbone = GoogleFeatures()\n",
    "backbone.out_channels = 192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasterrcnn_google(pretrained=False, progress=True,\n",
    "                            num_classes=2, pretrained_backbone=True,\n",
    "                             trainable_backbone_layers=3, **kwargs):\n",
    "    assert trainable_backbone_layers <= 5 and trainable_backbone_layers >= 0\n",
    "    # dont freeze any layers if pretrained model or backbone is not used\n",
    "    if not (pretrained or pretrained_backbone):\n",
    "        trainable_backbone_layers = 5\n",
    "    if pretrained:\n",
    "        # no need to download the backbone if pretrained is set\n",
    "        pretrained_backbone = False\n",
    "    model = FasterRCNN(backbone, num_classes, **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model():\n",
    "    model = fasterrcnn_google(pretrained=False)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, 2)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_device(train_device):\n",
    "    if train_device == Device.GPU:\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda:0\")\n",
    "        else:\n",
    "            raise ValueError(\"No GPU was found\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No GPU was found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-f32b9c4fcf74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_training_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-e965d15fe389>\u001b[0m in \u001b[0;36mget_training_device\u001b[0;34m(train_device)\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No GPU was found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No GPU was found"
     ]
    }
   ],
   "source": [
    "device = get_training_device(train_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initialize_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-281bb118eecb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = create_dataloaders_dictionary(dataloaders_arguments,input_size=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Some basic calculations that could be useful later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_size = len(dataloaders[\"train\"].dataset)\n",
    "number_of_iteration_per_epoch = int(train_dataset_size / dataloaders_arguments.batch_size)\n",
    "total_number_of_iteration = number_of_epochs * number_of_iteration_per_epoch\n",
    "learning_rate_step_size = 2 * number_of_iteration_per_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Prepare for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learnable_parameters(model, feature_extract):\n",
    "    params_to_update = model.parameters()\n",
    "\n",
    "    if feature_extract:\n",
    "        params_to_update = []\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                params_to_update.append(param)\n",
    "                print(\"\\t\", name)\n",
    "    return params_to_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_to_update = get_learnable_parameters(model, feature_extract=False)\n",
    "optimizer = optim.Adam(params_to_update, lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer,\n",
    "                                                              T_0=learning_rate_step_size,\n",
    "                                                              T_mult=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _save_model(model, model_path):\n",
    "    torch.save(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the checkpoint helps to starting training from a certain point.\n",
    "def _save_checkpoint(epoch, model, optimizer, checkpoint_path):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(epoch,model, optimizer):\n",
    "    model_path = \"/kaggle/working/best_model_epoch_{epoch}.pth\"\n",
    "    _save_model(model.state_dict(), model_path)\n",
    "    checkpoint_path = \"/kaggle/working/checkpoint_{epoch}.pth\"\n",
    "    _save_checkpoint(epoch, model, optimizer, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detector:\n",
    "    def fit_model(self):\n",
    "        start_epoch = 0\n",
    "        iteration_i = 0\n",
    "        for current_epoch in range(start_epoch, number_of_epochs):\n",
    "            running_loss = 0\n",
    "            print(f\"Starting Epoch: {current_epoch}\")\n",
    "            progress_bar = tqdm(dataloaders[\"train\"])\n",
    "            for inputs, labels in  progress_bar:\n",
    "                running_loss_i = self.training_round(inputs, labels)\n",
    "                running_loss += running_loss_i\n",
    "                current_running_error = running_loss/((iteration_i - \n",
    "                                                      current_epoch * \n",
    "                                                      number_of_iteration_per_epoch + 1)*batch_size)\n",
    "                progress_bar.set_description(f\"Running train loss: {current_running_error}\")\n",
    "                iteration_i += 1\n",
    "            epoch_loss = running_loss / len(dataloaders[\"train\"].dataset)\n",
    "            print(f\"Finishing Current epoch: {current_epoch} ... training loss: {epoch_loss}\")\n",
    "            print(\"saving the model and checkpoint: \")\n",
    "            save_model(current_epoch, model, optimizer)\n",
    "            for inputs, labels in tqdm(dataloaders[\"val\"]):\n",
    "                self.validation_round(inputs, labels)\n",
    "\n",
    "    def training_round(self, inputs, labels):\n",
    "        inputs = list(image.to(device) for image in inputs)\n",
    "        inputs = torch.stack(inputs)\n",
    "        labels = [{k: v.to(device) for k, v in t.items() if not isinstance(v, str)} for t in labels]\n",
    "        model.train()\n",
    "        loss_dict = model(inputs, labels)\n",
    "        loss = sum(loss for loss in loss_dict.values())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        lr_scheduler.step()\n",
    "        running_loss_i = loss.item() * inputs.size(0) \n",
    "        return running_loss_i\n",
    "\n",
    "    def validation_round(self, inputs, labels):\n",
    "        model.eval()\n",
    "        inputs = list(image.to(device) for image in inputs)\n",
    "        inputs = torch.stack(inputs)\n",
    "        labels = [{k: v.to(device) for k, v in t.items() if not isinstance(v, str)} for t in labels]\n",
    "        outputs = model(inputs)\n",
    "        outputs = [{k: v.to(\"cpu\") for k, v in t.items()} for t in outputs]\n",
    "        # Note: I used here MSCOCO evaluation metric locally. Unfortunately, I could not run in this kernel.\n",
    "        # I appreciate it if you can help here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector  =  Detector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/95 [00:08<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-5016cc496687>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## The model will be saved for each epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-d5cf2271cfd1>\u001b[0m in \u001b[0;36mfit_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mprogress_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m  \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                 \u001b[0mrunning_loss_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_round\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                 \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mrunning_loss_i\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 current_running_error = running_loss/((iteration_i - \n",
      "\u001b[0;32m<ipython-input-33-d5cf2271cfd1>\u001b[0m in \u001b[0;36mtraining_round\u001b[0;34m(self, inputs, labels)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtraining_round\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-d5cf2271cfd1>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtraining_round\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "## The model will be saved for each epoch\n",
    "detector.fit_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "**I appreciate your feedback and upvote if you think it was useful**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "5a9e26655e84484c993abde8522b45aa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "610a1775094944d0ab9fc034258fd959": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b07ffa410dfe4cfca543d21e98a1cec3",
       "max": 52147035.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_c469d32564404dc2b75abbe3b98bf41a",
       "value": 52147035.0
      }
     },
     "8cb5c22d94a74a05b0fde21880c82006": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ae113c736274478f8c10f63298fd4815": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_610a1775094944d0ab9fc034258fd959",
        "IPY_MODEL_b58e89b673e7405b8ebe31d307eaa4f8"
       ],
       "layout": "IPY_MODEL_b63b190c780a4c38816b3f33ea5c6f26"
      }
     },
     "b07ffa410dfe4cfca543d21e98a1cec3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b58e89b673e7405b8ebe31d307eaa4f8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5a9e26655e84484c993abde8522b45aa",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_8cb5c22d94a74a05b0fde21880c82006",
       "value": " 49.7M/49.7M [00:01&lt;00:00, 47.8MB/s]"
      }
     },
     "b63b190c780a4c38816b3f33ea5c6f26": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c469d32564404dc2b75abbe3b98bf41a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
